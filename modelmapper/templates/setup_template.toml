[settings]
null_values = ["\\n", "", "na", "unk", "null", "none", "nan", "1/0/00", "1/0/1900", "-"]  # Any string that should be considered null
boolean_true = ["true", "t", "yes", "y", "1"]  # Any string that should be considered boolean True
boolean_false = ["false", "f", "no", "n", "0"]  # Any string that should be considered boolean False
dollar_to_cent = true  # If true, then when a field is marked as money field, the values in csv will be multiplied by 100 to be stored as cents in integer field. Even if the original data is decimal.
percent_to_decimal = true  # If true, then when a field is marked as percent values, the csv values will be divided by 100 to be put in database. Example: 10 becomes 0.10
add_digits_to_decimal_field = 2  # This is for padding decimal fields. If the biggest decimal size in your training csvs is for example xx.xxx, then padding of 2 on each side will define a database field that can fit xxxx.xxxxx
add_to_string_length = 32  # Padding for string fields. If the biggest string length in your training csvs is X, then the db field size will be X + padding.
datetime_allowed_characters = "/:-"  # Special characters that are allowed in a datetime field. These characters are used to evaluate then that string value can be a datetime value.
datetime_formats = ["%m/%d/%y", "%m/%d/%Y", "%Y%m%d"]  # The list of any possible datetime formats in all your training csvs.
field_name_full_conversion = []  # Use this to tell ModelMapper which field names should be considered to be the same field. This is useful if you have field names changing across different csvs. Example: [['field 1', 'field a'], ['field 2', 'field b']]
field_name_part_conversion = [["#", "num"], [")", ""], ["(", ""], [":", "_"], [" ", "_"], ["/", "_"], [".", "_"], ["-", "_"], ["%", "_percent"], ["?", ""], ["!", ""], [",", ""], ["'", ""], ["&", "_and_"], ["@", "_at_"], ["$", "_dollar_"], [">=", "_bigger_or_equal_"], [">", "_bigger_"], ["<=", "_less_or_equal_"], ["<", "_less_"], ["=", "_equal_"], ["___", "_"], ["__", "_"]]  # list of words in field name that should be replaced by another word.
default_value_for_field_when_casting_error = ""  # Python dictionary of field names that have default values to be used when casting fails. This is only used during ETL cleaning jobs not during training phase. Example: "{'field_name':None}" will give the field_name value of Null if type casting fails during the cleaning phase. That way the job can continue running instead of crashing.
dollar_value_if_word_in_field_name = []  # If the field name has any of these words, consider it as money field. It only matters if dollar_to_cent is True
non_string_fields_are_all_nullable = true  # If yes, any non string field will be automatically nullable. Otherwise only if you have null values in your training csv, then it will be marked as nullable.
string_fields_can_be_nullable = false  # Normally string fields should not be nullable since they can be just empty. If you set it to True, then if there are null values inside the string field in any of the training csvs, it will mark the field is nullable.
should_reprocess = false  # Whether to reprocess files that are already processed or not. The recommended value is false so we avoid reprocessing files that are already processed before.
training_csvs = []  # The list of relative paths to the training csvs
output_model_file = ""  # The relative path to the ORM model file that the output generated model will be inserted into.
ignore_lines_that_include_only_subset_of = ["", "-"]  # Ignore lines that only include these characters
ignore_fields_in_signature_calculation = ["id", "raw_key_id"]  # Only used when ignore_duplicate_rows_when_importing is true. Ignore these field names when calculating the signature of the row for avoiding duplicate data. Only used when importing the data into database and NOT for training the model.
ignore_duplicate_rows_when_importing = true  # If true, calculate the signature (hash) for each row when importing and avoid inserting the row if the signature already exists.
encrypt_raw_data_during_backup = true  # If true, encrypt the raw data received by the client before backing it up.
decrypt_raw_data = false  # If true it will try to decrypt raw data.
delete_source_object_after_backup = false  # If true and the client supports this parameter, the client will try to delete the source object once it is downloaded and backed up.
fields_to_be_encrypted = []  # List of field names that need to be encrypted.
fields_to_be_scrubbed = []  # List of field names that need to be scrubbed. Note that the field will be scrubbed before the type is inferred and thus it will be inferred as boolean field.
slack_username = ""  # slack username for reporting errors when importing data.
slack_channel = ""  # slack channel for reporting errors when importing data.
slack_http_endpoint = ""  # slack http endpoint for reporting errors when importing data. It can be an environment variable.
slack_handle_to_ping = ""  # slack handle to ping in the channel in case of errors. For example: @some_user or @here
identify_header_by_column_names = []  # If the first row of the sheet is not the headers row, you can use this list to define a subset of raw column names that can be used to find the row that has the headers. For example if there are a few rows with descriptions and then on row X which includes column names like FirstName and LastName, then you set this attribute to ["FirstName", "LastName"]

[settings.max_int]
SmallInteger = 32767  # An integer field with ALL numbers below this in your training csv will be marked as SmallInteger. If you don't want any SmallIntegerfields, then remove this line.
Integer = 2147483647  # An integer field with ALL numbers below this but at least one above SmallInteger in your training csv will be marked as Integer
BigInteger = 9223372036854775807  # An integer field with ALL numbers below this but at least one above Integer in your training csv will be marked as BigInteger
